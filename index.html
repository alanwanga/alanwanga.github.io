<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Work Portfolio</title>
    <script src="https://cdn.tailwindcss.com"></script>
  </head>
  <body class="bg-gray-900 text-gray-200 font-sans leading-relaxed px-4 sm:px-6 lg:px-12 max-w-5xl mx-auto text-base">
    <header class="flex flex-col sm:flex-row items-start sm:items-center gap-4 my-8">
      <img src="https://avatars0.githubusercontent.com/u/3997703?s=200" alt="Taoyuan Wang" class="rounded-full w-16 h-16 sm:w-20 sm:h-20" />
      <div>
        <h1 class="text-3xl font-bold">Work Portfolio</h1>
        <p class="text-gray-400 text-sm sm:text-base ml-1">taoyuanw@google.com</p>
      </div>
    </header>

    <main>
      <section>
        <h2 class="text-white text-2xl font-semibold mt-10 mb-4">Professional Overview</h2>
        <p class="my-4">My name is Taoyuan Wang. I have 7+ years of industry experience in software development, specializing in autonomous systems. I hold both Master's and Bachelor's degrees in Computer Science.</p>
        <p class="my-4">Currently, I’m a backend engineer at Wing, an Alphabet moonshot from Google X. I work on the Uncrewed Traffic Management (UTM) team, where we are building Google Maps for the sky to help autonomous aircrafts safely navigate. Our system determines when and where drones can fly, enabling a reliable and scalable delivery network.</p>
        <p class="my-4">I have been granted Google C++ readability and received a National Interest Waiver (NIW) in recognition of my contributions to Highly Automated, Autonomous, and Uncrewed Systems (UxS) and Robotics. My work focuses on building and optimizing the infrastructures that power autonomous systems.</p>
        <p class="my-4">Previously, I was a full-stack developer at Appen, where I led the design and development of 3D point cloud annotation tools for autonomous vehicles. My work supported 3D visualization and multi-sensor fusion across lidar, radar, and camera data to produce large-scale, high-quality training datasets for machine learning.</p>
      </section>

      <section>
        <h2 class="text-white text-2xl font-semibold mt-10 mb-4">Selected Projects</h2>

        <div class="bg-gray-800 rounded-lg p-6 my-6 shadow-md">
          <h3 class="text-lg font-semibold text-white">Wing OpenSky Operator Interfaces</h3>
          <p class="my-2">The interfaces enable operators to oversee and control multiple aircraft safely and efficiently. Integrated with Google Maps and internal data sources, the systems provide real-time aircraft location tracking, flight path visualization, and compliance monitoring in accordance with FAA and other international regulatory guidelines. The systems enable timely four-dimensional (3D plus time) airspace restriction enforcement, ensuring adherence to safety protocols.</p>
          <p class="my-2">I developed the backend components for OpenSky, working closely with product managers to translate requirements into design docs. I implemented the backend handlers and validators to ensure compliance with regulatory standards and operational safety. Additionally, I coordinated planning, task allocation, and testing efforts, contributing to robust UTM systems that significantly increased operator capacity and supported the scaling of autonomous delivery services.</p>
          <p class="my-2">
             <a href="https://opensky.wing.com/visitor/map?lat=-28.418053&lng=133.454211&zoom=4" target="_blank" rel="noopener noreferrer" class="text-blue-400 hover:text-purple-300 underline">OpenSky - Online Preview</a>
            <iframe class="w-full h-[32rem] rounded-md mt-2" src="https://opensky.wing.com/visitor/map?lat=-28.418053&lng=133.454211&zoom=4"></iframe>
          </p>
        </div>

        <div class="bg-gray-800 rounded-lg p-6 my-6 shadow-md">
          <h3 class="text-lg font-semibold text-white">Wing Operational Configuration Systems</h3>
          <p class="my-2">The systems, built on Alphabet’s dynamic server configuration push infrastructures, empower Solution Managers to manage compliance-related flight rules for multinational operations. The systems enable automated enforcement of geographical flight restrictions, ensure that operators hold up-to-date certifications, and also replace our previous authentication modules based on Google Zanzibar and Baggins with role-based configurations and permission checking optimized for low-latency applications, such as pilot-issued emergency landing commands.</p>
          <p class="my-2">I led the design and implementation of configuration schemas and validation mechanisms, preventing user errors while ensuring semantic correctness. Additionally, I integrated the systems as a module across multiple components to support rapid, reliable deployment of configuration updates. This work significantly enhanced the compliance, scalability, and operational efficiency of autonomous drone operations.</p>
        </div>

        <div class="bg-gray-800 rounded-lg p-6 my-6 shadow-md">
          <h3 class="text-lg font-semibold text-white">Gemini Coordination Simulator</h3>
          <p class="my-2">Developed a centralized LLM/VLA coordination system that allows users to control multiple drones using natural language, enabling them to perform general-purpose tasks without needing pre-programmed instructions. The system fuses real-time drone sensor data with object detection and tracking to build a rich, shared understanding of the environment. Natural language inputs are processed to contextualize human intent, which is then translated into executable drone tasks. An integrated 3D visualization and simulation interface supports intuitive human-AI-robot interaction, making it easier to test, monitor, and guide multi-agent behavior.</p>
          <p class="my-2">To accelerate development and testing, the system can also generate custom 3D environments from simple text descriptions, significantly reducing the engineering effort for scenario creation. All interactions, including user commands, AI reasoning, drone actions, and human feedback, are logged to build a comprehensive dataset for reinforcement learning, enabling continuous performance improvement. Designed for real-world deployment, the coordinator can interface with physical drones through low-latency control models, paving the way for advanced applications in autonomous delivery, disaster response, and multi-robot collaboration.</p>
          <p class="my-2">
            <a href="https://www.youtube.com/watch?v=Z8eXKve0Vs8" target="_blank" rel="noopener noreferrer" class="text-blue-400 hover:text-purple-300 underline">Demo - Gemini for robotics and embodied intelligence</a>
            <iframe class="w-full h-[32rem] rounded-md mt-2" src="https://www.youtube.com/embed/Z8eXKve0Vs8?si=AXKCfBPQ6bgav2tJ" allowfullscreen></iframe>
          </p>
        </div>

        <div class="bg-gray-800 rounded-lg p-6 my-6 shadow-md">
          <h3 class="text-lg font-semibold text-white">Appen AI-Assisted 3D Point Cloud Annotation Tools</h3>
          <p class="my-2">The tools enhance the perception capabilities of autonomous systems by allowing precise labeling of 3D point cloud data from LiDAR and radar sensors. Features like semantic segmentation enable annotators to classify individual points, improving training data quality for ML models.</p>
          <p class="my-2">I led the full-stack development of these tools from prototype to production, integrating real-time AI-assisted annotation via point cloud clustering. This approach significantly improved efficiency and accuracy, reducing annotation time and costs for autonomous vehicle and robotics perception models.</p>
          <p class="my-2">
            <a href="https://www.youtube.com/watch?v=gDQWW3u_2AI" target="_blank" rel="noopener noreferrer" class="text-blue-400 hover:text-purple-300 underline">Tech Day Demo - LiDAR Annotation</a>
            <iframe class="w-full h-64 rounded-md mt-2" src="https://www.youtube.com/embed/gDQWW3u_2AI?si=3aUpdLVPnmq6ril5" allowfullscreen></iframe>
          </p>
          <p class="my-2">
            <a href="https://www.youtube.com/watch?v=LFIJ32s54aY" target="_blank" rel="noopener noreferrer" class="text-blue-400 hover:text-purple-300 underline">Tech Day Demo - LiDAR Lane Line Segmentation</a>
            <iframe class="w-full h-64 rounded-md mt-2" src="https://www.youtube.com/embed/LFIJ32s54aY?si=i3zB2CtwN7AtoU2X" allowfullscreen></iframe>
          </p>
        </div>

        <div class="bg-gray-800 rounded-lg p-6 my-6 shadow-md">
          <h3 class="text-lg font-semibold text-white">Appen Sensor Fusion Experiments</h3>
          <p class="my-2">Sensor fusion integrates data from LiDAR, radar, and cameras to enhance multimodal annotation, object detection, and tracking, which are crucial for self-driving cars and unmanned aircrafts.</p>
          <p class="my-2">I identified various camera models used in autonomous systems, developed sensor fusion and visualization scripts, and ensured compatibility between client coordinate systems and our fusion algorithms. My work enabled real-time projection of 3D points into the 2D domain, enhancing annotation accuracy and facilitating better environmental understanding for annotators.</p>
        </div>
      </section>

      <section>
        <h2 class="text-white text-2xl font-semibold mt-10 mb-4">Key Achievements</h2>

        <div class="bg-gray-800 rounded-lg p-6 my-6 shadow-md">
          <h3 class="text-lg font-semibold text-white">Wing</h3>
          <p class="my-2">I addressed the challenge of integrating autonomous drones into regulated airspace. I developed the configuration systems and backend for OpenSky, Google Maps-based operational interfaces, allowing operators to monitor multiple drones efficiently. Previously, one operator could only oversee one aircraft, creating a bottleneck in scaling our service. My contributions met regulatory requirements from agencies such as FAA and CASA, enhancing the scalability and safety of our drone delivery operations.</p>
          <p class="my-2">
            <a href="https://wing.com/opensky" target="_blank" rel="noopener noreferrer" class="text-blue-400 hover:text-purple-300 underline">OpenSky - Our Drone Airspace Platform</a>
            <iframe class="w-full h-[32rem] rounded-md mt-2" src="https://wing.com/opensky"></iframe>
          </p>
        </div>

        <div class="bg-gray-800 rounded-lg p-6 my-6 shadow-md">
          <h3 class="text-lg font-semibold text-white">Appen</h3>
          <p class="my-2">As the lead developer of 3D point cloud annotation tools, I aimed to reduce annotation costs through machine learning. Initially, state-of-the-art object detection models required extensive labeling, limiting their effectiveness. To improve efficiency, I implemented a real-time AI-assisted interaction inspired by academic research. Annotators could click on the point cloud, triggering DBSCAN clustering to generate a bounding box. This approach improved annotation efficiency by 35%.</p>
          <div class="flex flex-col sm:flex-row justify-center items-center gap-6 mt-4">
            <img src="./3D.gif" alt="Example 1" loading="lazy" class="w-full sm:w-1/2 max-w-md rounded-lg shadow-md" />
            <img src="./4D.gif" alt="Example 2" loading="lazy" class="w-full sm:w-1/2 max-w-md rounded-lg shadow-md" />
          </div>
        </div>
      </section>

      <footer class="mt-16 text-sm text-center">
        <a href="https://alanwanga.github.io/TaoyuanWang_resume.pdf" target="_blank" class="text-blue-400 hover:text-purple-400 underline">View Resume</a> for contact details<br> — I’d be glad to hear from you!<br></br>
      </footer>
    </main>
  </body>
</html>