<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c0{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Arial";font-style:normal}.c4{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c10{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c9{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c6{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#0000ee;text-decoration:underline}.c11{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c7{color:inherit;text-decoration:inherit}.c3{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c11 doc-content"><p class="c5 title" id="h.uvbr8xueem4u"><span class="c10">Work Portfolio</span></p><p class="c2"><span class="c6"><a class="c7" href="mailto:alanwanga@gmail.com">Taoyuan Wang</a></span><span>&nbsp;</span><span>2025-04-30</span></p><h1 class="c4" id="h.sgp9lbh4rfzw"><span class="c8">Professional Overview</span></h1><p class="c2"><span class="c1">I have 7+ years of industry experience in software development, specializing in autonomous systems. I hold both Master&#39;s and Bachelor&#39;s degrees in Computer Science.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c1">Currently, I am a backend engineer on the Uncrewed Traffic Management (UTM) team at Wing, an Alphabet moonshot project initiated by Google X Development. I am working on a new variation of Google Maps in 3D airspace for autonomous aircrafts. Our systems determine when and where drones are allowed to fly, ensuring safe access to the sky.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c1">I have been granted Google C++ readability and received a National Interest Waiver (NIW) in recognition of my impactful contributions to Highly Automated, Autonomous, and Uncrewed Systems (UxS) and Robotics. My work focuses on building and optimizing the infrastructures that power autonomous systems.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c1">Previously, I was a full-stack developer on the Computer Vision Tools team at Appen, where we provided high-quality training data for ML models at scale. I led the design and development of 3D point cloud annotation tools for autonomous vehicles and robots, supporting 3D visualization and sensor fusion across LiDAR, radar, and camera data.</span></p><h1 class="c4" id="h.vawcludyjlhg"><span class="c8">Selected Projects</span></h1><p class="c2"><span class="c0">Wing OpenSky Operator Interfaces</span></p><p class="c2"><span class="c1">The OpenSky operator interfaces enable operators to oversee and control multiple aircraft safely and efficiently. Integrated with Google Maps and internal data sources, the systems provide real-time aircraft location tracking, flight path visualization, and compliance monitoring in accordance with FAA and other international regulatory guidelines. The systems enable timely four-dimensional (3D plus time) airspace restriction enforcement, ensuring adherence to safety protocols.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c1">I developed backend components for OpenSky, working closely with product managers to translate requirements into design documents. I implemented backend handlers and validators to ensure compliance with regulatory standards and operational safety. Additionally, I coordinated planning, task allocation, and testing efforts, contributing to robust UTM systems that significantly increased operator capacity and supported the scaling of autonomous delivery services.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c0">Wing Operational Configuration Systems</span></p><p class="c2"><span>The operational configuration systems, built on Alphabet&rsquo;s dynamic server configuration push infrastructures</span><span class="c1">, empower Solution Managers to manage compliance-related flight rules for multinational operations. The systems enable automated enforcement of geographical flight restrictions, ensure that operators hold up-to-date certifications, and also replace our previous authentication modules based on Google Zanzibar and Baggins with role-based configurations and permission checking optimized for low-latency applications, such as pilot-issued emergency landing commands.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c1">I led the design and implementation of configuration schemas and validation mechanisms, preventing user errors while ensuring semantic correctness. Additionally, I integrated the systems as a module across multiple components to support rapid, reliable deployment of configuration updates. This work significantly enhanced the compliance, scalability, and operational efficiency of autonomous drone operations.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c0">Appen AI-Assisted 3D Point Cloud Annotation Tools</span></p><p class="c2"><span class="c1">These tools enhance the perception capabilities of autonomous systems by allowing precise labeling of 3D point cloud data from LiDAR and radar sensors. Features like semantic segmentation enable annotators to classify individual points, improving training data quality for ML models.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c1">I led the full-stack development of these tools from prototype to production, integrating real-time AI-assisted annotation via point cloud clustering. This approach significantly improved efficiency and accuracy, reducing annotation time and costs for autonomous vehicle and robotics perception models.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c0">Appen Sensor Fusion Experiments</span></p><p class="c2"><span class="c1">Sensor fusion integrates data from LiDAR, radar, and cameras to enhance multimodal annotation, object detection, and tracking, which are crucial for self-driving cars and unmanned aircrafts.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c1">I identified various camera models used in autonomous systems, developed sensor fusion and visualization scripts, and ensured compatibility between client coordinate systems and our fusion algorithms. My work enabled real-time projection of 3D points into the 2D domain, enhancing annotation accuracy and facilitating better environmental understanding for annotators.</span></p><h1 class="c4" id="h.hlhgoh9cvcp0"><span class="c8">Key Achievements</span></h1><p class="c2"><span class="c0">Wing</span></p><p class="c2"><span>At Wing, I addressed the challenge of integrating autonomous drones into regulated airspace. I developed the configuration systems and backend for OpenSky [1][2], Google Maps-based operational interfaces, allowing operators to monitor multiple drones efficiently. Previously, one operator could only oversee one aircraft, creating a bottleneck in scaling our service. My contributions met regulatory requirements from agencies such </span><span>as FAA</span><span class="c1">&nbsp;and CASA, enhancing the scalability and safety of our drone delivery operations.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span>[1] </span><span class="c9"><a class="c7" href="https://www.google.com/url?q=https://wing.com/opensky&amp;sa=D&amp;source=editors&amp;ust=1747017646298456&amp;usg=AOvVaw0-D4NRv3fsfX7MD3V81lzQ">OpenSky - Our Drone Airspace Platform</a></span></p><p class="c2"><span>[2] </span><span class="c9"><a class="c7" href="https://www.google.com/url?q=https://opensky.wing.com/visitor/map?lat%3D-28.418053%26lng%3D133.454211%26zoom%3D4&amp;sa=D&amp;source=editors&amp;ust=1747017646298847&amp;usg=AOvVaw39DK97pt-r9BZNVcQXj5ck">OpenSky - Online Preview</a></span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span class="c0">Appen</span></p><p class="c2"><span class="c1">As the lead developer of Appen&#39;s 3D point cloud annotation tools [1][2], I aimed to reduce annotation costs through machine learning. Initially, state-of-the-art object detection models required extensive labeling, limiting their effectiveness. To improve efficiency, I implemented a real-time AI-assisted interaction inspired by academic research. Annotators could click on the point cloud, triggering DBSCAN clustering to generate a bounding box. This approach improved annotation efficiency by 35%.</span></p><p class="c2 c3"><span class="c1"></span></p><p class="c2"><span>[1] </span><span class="c6"><a class="c7" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DgDQWW3u_2AI%26ab_channel%3DAppen&amp;sa=D&amp;source=editors&amp;ust=1747017646300434&amp;usg=AOvVaw3w3MveKiGPjjAhUIoy6FTU">Tech Day Demo - LiDAR Annotation</a></span></p><p class="c2"><span>[2] </span><span class="c6"><a class="c7" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DLFIJ32s54aY%26ab_channel%3DAppen&amp;sa=D&amp;source=editors&amp;ust=1747017646300666&amp;usg=AOvVaw2MEMr5hdkZ8k40zdWXu8pb">Tech Day Demo - LiDAR Lane Line Segmentation</a></span></p></body></html>
